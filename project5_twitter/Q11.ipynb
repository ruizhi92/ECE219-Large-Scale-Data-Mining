{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import pytz\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from statistics import mean\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statistics import mean\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"tweet_data/\"\n",
    "\n",
    "files = [\"tweets_#gohawks.txt\", \"tweets_#gopatriots.txt\", \\\n",
    "        \"tweets_#nfl.txt\", \"tweets_#patriots.txt\", \\\n",
    "        \"tweets_#sb49.txt\", \"tweets_#superbowl.txt\"]\n",
    "topics = [\"gohawks\", \"gopatriots\", \"nfl\", \"patriots\", \"sb49\", \"superbowl\"]\n",
    "test_files =[\"sample0_period1.txt\",\"sample0_period2.txt\",\"sample0_period3.txt\", \\\n",
    "             \"sample1_period1.txt\",\"sample1_period2.txt\",\"sample1_period3.txt\",\"sample2_period1.txt\",\\\n",
    "             \"sample2_period2.txt\",\"sample2_period3.txt\"]\n",
    "\n",
    "for i, fl in enumerate(files):\n",
    "    print(\"files[\" + str(i) + \"] => \" + fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(file):\n",
    "    \"\"\"\n",
    "    Prase x conponent of the dataset into pandas DataFrame including columns of:\n",
    "    tweets, retweets, total_followers, max_followers, hour of the day\n",
    "    with lines of hours,\n",
    "    Prase y of dataset as number of tweets in the next hour.\n",
    "    \"\"\"\n",
    "   \n",
    "    # extract raw features\n",
    "    pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "    data_raw = []\n",
    "    for line in open(path + file, 'r', encoding=\"utf-8\") :\n",
    "        row_tmp = []\n",
    "        a = json.loads(line)\n",
    "        time = a['citation_date']\n",
    "        retweet = a['metrics']['citations']['total']\n",
    "        foll = a['author']['followers']        \n",
    "        row_tmp.append(time)\n",
    "        row_tmp.append(retweet)\n",
    "        row_tmp.append(foll)        \n",
    "        data_raw.append(row_tmp)\n",
    "    \n",
    "    # sort according to time\n",
    "    pddata_raw = pd.DataFrame(data_raw,columns=['time','retweets','followers'])\n",
    "    pddata_raw = pddata_raw.sort_values(by = 'time')\n",
    "    pddata_raw = pddata_raw.reset_index(drop=True)\n",
    "    pddata_raw['tweets'] = 1    \n",
    "#     print(pddata_raw)\n",
    "\n",
    "    # reset time to hour index\n",
    "    hour_accu = []\n",
    "    hour_day = []\n",
    "    for index, row in pddata_raw.iterrows():  \n",
    "        p = datetime.datetime.fromtimestamp(row[\"time\"], pst_tz)  \n",
    "        hour_accu.append(((p.month-1)*31+p.day-14)*24+p.hour)\n",
    "        hour_day.append(p.hour)    \n",
    "    pddata_raw[\"time\"] = hour_accu\n",
    "    pddata_raw[\"hour of day\"] = hour_day\n",
    "    \n",
    "    # create a new dataframe with desired form\n",
    "    df = pd.DataFrame([],columns=['hour index','tweets','retweets','followers sum','followers max','hour of day'])\n",
    "    df['hour index'] = range(pddata_raw.iloc[len(pddata_raw.index)-1,0]+1)\n",
    "    df['tweets'] = pddata_raw.groupby(\"time\")['tweets'].sum()\n",
    "    df['retweets'] = pddata_raw.groupby(\"time\")['retweets'].sum()\n",
    "    df['followers sum'] = pddata_raw.groupby(\"time\")['followers'].sum()\n",
    "    df['followers max'] = pddata_raw.groupby(\"time\")[\"followers\"].max()\n",
    "    df['hour of day'] = pddata_raw.groupby(\"time\")['hour of day'].mean()\n",
    "    df = df.drop([0]).fillna(0).reset_index(drop=True)\n",
    "\n",
    "    # assign number of tweets of the next hour to be the target value\n",
    "    df_y = df.iloc[1:,1].reset_index(drop=True)\n",
    "    df = df[:len(df_y)]\n",
    "    \n",
    "    return df.iloc[:,1:],df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset_ten_features(file):\n",
    "    \"\"\"\n",
    "    Prase x conponent of the dataset into pandas DataFrame including columns of:\n",
    "    tweets, retweets, total_followers, max_followers, mentioned, media, active, author, favourites_count, title\n",
    "    with lines of hours,\n",
    "    where mentioned: number of @ in tweets per hour\n",
    "          media: number of url in tweets per hour\n",
    "          active: a measure of active state of author\n",
    "          author: number of unique authors post tweet per hour\n",
    "          favourites_count: the total number of likes by this user\n",
    "          title: length of this tweet's title\n",
    "    Prase y of dataset as number of tweets in the next hour.\n",
    "    \"\"\"\n",
    "   \n",
    "    # extract raw features\n",
    "    pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "    data_raw = []\n",
    "    for line in open(path + file, 'r', encoding=\"utf-8\") :\n",
    "        row_tmp = []\n",
    "        a = json.loads(line)\n",
    "        time = a['citation_date']\n",
    "        retweet = a['metrics']['citations']['total']\n",
    "        foll = a['author']['followers']             \n",
    "        ment = len(a['tweet']['entities']['user_mentions'])        \n",
    "        medi = len(a['tweet']['extended_entities']['media']) if 'extended_entities' in a['tweet'] else 0\n",
    "        hist_tw = a['tweet']['user'][\"statuses_count\"]\n",
    "        hist_yr = a['tweet']['user']['created_at'][-4:]\n",
    "        acti = hist_tw/(2015-float(hist_yr)+1) \n",
    "        auth = a['author']['name']\n",
    "        favo = a['tweet']['user']['favourites_count']\n",
    "        titl = len(a['title'])\n",
    "        \n",
    "        # append to list\n",
    "        row_tmp.append(time)\n",
    "        row_tmp.append(retweet)\n",
    "        row_tmp.append(foll)    \n",
    "        row_tmp.append(ment) \n",
    "        row_tmp.append(medi) \n",
    "        row_tmp.append(acti)  \n",
    "        row_tmp.append(auth)\n",
    "        row_tmp.append(favo)\n",
    "        row_tmp.append(titl)        \n",
    "        data_raw.append(row_tmp)\n",
    "    \n",
    "    # sort according to time\n",
    "    pddata_raw = pd.DataFrame(data_raw,columns=['time','retweets','followers','mentioned','media',\\\n",
    "                                                'active','author','favourites_count','title'])\n",
    "    pddata_raw = pddata_raw.sort_values(by = 'time')\n",
    "    pddata_raw = pddata_raw.reset_index(drop=True)\n",
    "    pddata_raw['tweets'] = 1                   \n",
    "#     print(pddata_raw)\n",
    "\n",
    "    # reset time to hour index\n",
    "    hour_accu = []\n",
    "    hour_day = []\n",
    "    for index, row in pddata_raw.iterrows():  \n",
    "        p = datetime.datetime.fromtimestamp(row[\"time\"], pst_tz)  \n",
    "        hour_accu.append(((p.month-1)*31+p.day-14)*24+p.hour)\n",
    "        hour_day.append(p.hour)    \n",
    "    pddata_raw[\"time\"] = hour_accu\n",
    "    pddata_raw[\"hour of day\"] = hour_day\n",
    "    \n",
    "    # create a new dataframe with desired form\n",
    "    df = pd.DataFrame([],columns=['hour index','tweets','retweets','followers sum','followers max',\\\n",
    "                                  'mentioned','media','active','author','favourites_count','title'])\n",
    "    df['hour index'] = range(pddata_raw.iloc[len(pddata_raw.index)-1,0]+1)\n",
    "    df['tweets'] = pddata_raw.groupby(\"time\")['tweets'].sum()\n",
    "    df['retweets'] = pddata_raw.groupby(\"time\")['retweets'].sum()\n",
    "    df['followers sum'] = pddata_raw.groupby(\"time\")['followers'].sum()\n",
    "    df['followers max'] = pddata_raw.groupby(\"time\")[\"followers\"].max()\n",
    "    df['mentioned'] = pddata_raw.groupby(\"time\")['mentioned'].sum()\n",
    "    df['media'] = pddata_raw.groupby(\"time\")['media'].sum()\n",
    "    df['active'] = pddata_raw.groupby(\"time\")['active'].mean()  \n",
    "    df['author'] = pddata_raw.groupby(\"time\")['author'].nunique() # count number of not-repeating authors    \n",
    "    df['favourites_count'] = pddata_raw.groupby(\"time\")['favourites_count'].sum()\n",
    "    df['title'] = pddata_raw.groupby(\"time\")['title'].mean()\n",
    "            \n",
    "    # reset index of df\n",
    "    df = df.drop([0]).fillna(0).reset_index(drop=True)\n",
    "\n",
    "    # assign number of tweets of the next hour to be the target value\n",
    "    df_y = df.iloc[1:,1].reset_index(drop=True)\n",
    "    df = df[:len(df_y)]\n",
    "    \n",
    "    return df.iloc[:,1:],df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recipe(df_y, pred_y):\n",
    "    \"\"\"\n",
    "    This function plots fitted values vs true values\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    area = np.pi * (4)**2/4\n",
    "    plt.scatter(df_y, pred_y, s = area)\n",
    "    plt.plot([df_y.min(), df_y.max()], [df_y.min(), df_y.max()], 'k--', lw = 1)\n",
    "    plt.xlabel('true values')\n",
    "    plt.ylabel('fitted values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ols_regression(df, df_y):\n",
    "    X2 = sm.add_constant(df)\n",
    "    y = df_y.as_matrix()\n",
    "    lm = sm.OLS(y, X2).fit()\n",
    "    print(lm.summary())\n",
    "    print(list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate all data \n",
    "df, df_y = parse_dataset_ten_features(files[0])\n",
    "for i in range(1,6):\n",
    "    df_temp,df_temp_y=parse_dataset_ten_features(files[i])\n",
    "    df=df.append(df_temp,ignore_index=True)\n",
    "    df_y=df_y.append(df_temp_y,ignore_index=True)\n",
    "    print(df_temp.shape,df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={\n",
    "'max_depth': [10, 20, 40, 60, 80, 100, 200, None],\n",
    "'max_features': ['auto', 'sqrt'],\n",
    "'min_samples_leaf': [1, 2, 4],\n",
    "'min_samples_split': [2, 5, 10],\n",
    "'n_estimators': [200, 400, 600, 800, 1000,\n",
    "1200, 1400, 1600, 1800, 2000]\n",
    "}\n",
    "kf = KFold(n_splits=5,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units=[50,100,200,300,500,600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avg_RMSE2_train=np.zeros(6)\n",
    "avg_RMSE2_test=np.zeros(6)\n",
    "for n1,hidden_layer_sizes in enumerate(hidden_units):\n",
    "    MSE_train=[]\n",
    "    MSE_test=[]\n",
    "    total_train=0\n",
    "    total_test=0\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        X_train= df.iloc[train_index]\n",
    "        y_train= df_y.iloc[train_index]\n",
    "        X_test= df.iloc[test_index]\n",
    "        y_test= df_y.iloc[test_index]\n",
    "        reg = MLPRegressor(hidden_layer_sizes=(hidden_layer_sizes,),activation='relu', solver='adam', alpha=1e-5, random_state=42)            \n",
    "        reg.fit(X_train,y_train)\n",
    "        pred_train = reg.predict(X_train)\n",
    "        pred_test = reg.predict(X_test)\n",
    "        MSE_train.append(mean_squared_error(y_train, pred_train)*len(train_index))\n",
    "        MSE_test.append(mean_squared_error(y_test, pred_test)*len(test_index))\n",
    "        total_train=total_train+len(train_index)\n",
    "        total_test=total_test+len(test_index)\n",
    "    avg_RMSE2_test[n1]=np.sqrt(sum(MSE_test)/total_test)\n",
    "    avg_RMSE2_train[n1]=np.sqrt(sum(MSE_train)/total_train)\n",
    "    print(\"hidden layer sizes=\",hidden_layer_sizes)\n",
    "    print(\"RMSE_train=\",avg_RMSE2_train[n1],\"RMSE_test=\",avg_RMSE2_test[n1])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min RMSE in testset=\",np.min(avg_RMSE2_test))\n",
    "print(\"hidden layer sizes=\",hidden_units[np.argmin(avg_RMSE2_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = MLPRegressor(hidden_layer_sizes=(hidden_units[np.argmin(avg_RMSE2_test)],),activation='relu', solver='adam', alpha=1e-5, random_state=42) \n",
    "reg.fit(df,df_y)\n",
    "pred = reg.predict(df)\n",
    "print(\"MSE=\",mean_squared_error(df_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network with Standardscalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df)\n",
    "df_trans=scaler.transform(df)\n",
    "reg = MLPRegressor(hidden_layer_sizes=(hidden_units[np.argmin(avg_RMSE2_test)],),activation='relu', solver='adam', alpha=1e-5, random_state=42) \n",
    "reg.fit(df_trans,df_y)\n",
    "pred = reg.predict(df)\n",
    "print(\"MSE=\",mean_squared_error(df_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_regression(df, df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
