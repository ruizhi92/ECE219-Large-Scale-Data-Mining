{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import pytz\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from statistics import mean\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statistics import mean\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files[0] => tweets_#gohawks.txt\n",
      "files[1] => tweets_#gopatriots.txt\n",
      "files[2] => tweets_#nfl.txt\n",
      "files[3] => tweets_#patriots.txt\n",
      "files[4] => tweets_#sb49.txt\n",
      "files[5] => tweets_#superbowl.txt\n"
     ]
    }
   ],
   "source": [
    "path = \"tweet_data/\"\n",
    "\n",
    "files = [\"tweets_#gohawks.txt\", \"tweets_#gopatriots.txt\", \\\n",
    "        \"tweets_#nfl.txt\", \"tweets_#patriots.txt\", \\\n",
    "        \"tweets_#sb49.txt\", \"tweets_#superbowl.txt\"]\n",
    "topics = [\"gohawks\", \"gopatriots\", \"nfl\", \"patriots\", \"sb49\", \"superbowl\"]\n",
    "test_files =[\"sample0_period1.txt\",\"sample0_period2.txt\",\"sample0_period3.txt\", \\\n",
    "             \"sample1_period1.txt\",\"sample1_period2.txt\",\"sample1_period3.txt\",\"sample2_period1.txt\",\\\n",
    "             \"sample2_period2.txt\",\"sample2_period3.txt\"]\n",
    "\n",
    "for i, fl in enumerate(files):\n",
    "    print(\"files[\" + str(i) + \"] => \" + fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset_ten_features(file):\n",
    "    \"\"\"\n",
    "    Prase x conponent of the dataset into pandas DataFrame including columns of:\n",
    "    tweets, retweets, total_followers, max_followers, mentioned, media, active, author, favourites_count, title\n",
    "    with lines of hours,\n",
    "    where mentioned: number of @ in tweets per hour\n",
    "          media: number of url in tweets per hour\n",
    "          active: a measure of active state of author\n",
    "          author: number of unique authors post tweet per hour\n",
    "          favourites_count: the total number of likes by this user\n",
    "          title: length of this tweet's title\n",
    "    Prase y of dataset as number of tweets in the next hour.\n",
    "    \"\"\"\n",
    "   \n",
    "    # extract raw features\n",
    "    pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "    data_raw = []\n",
    "    for line in open(path + file, 'r', encoding=\"utf-8\") :\n",
    "        row_tmp = []\n",
    "        a = json.loads(line)\n",
    "        time = a['citation_date']\n",
    "        retweet = a['metrics']['citations']['total']\n",
    "        foll = a['author']['followers']             \n",
    "        ment = len(a['tweet']['entities']['user_mentions'])        \n",
    "        medi = len(a['tweet']['extended_entities']['media']) if 'extended_entities' in a['tweet'] else 0\n",
    "        hist_tw = a['tweet']['user'][\"statuses_count\"]\n",
    "        hist_yr = a['tweet']['user']['created_at'][-4:]\n",
    "        acti = hist_tw/(2015-float(hist_yr)+1) \n",
    "        auth = a['author']['name']\n",
    "        favo = a['tweet']['user']['favourites_count']\n",
    "        titl = len(a['title'])\n",
    "        \n",
    "        # append to list\n",
    "        row_tmp.append(time)\n",
    "        row_tmp.append(retweet)\n",
    "        row_tmp.append(foll)    \n",
    "        row_tmp.append(ment) \n",
    "        row_tmp.append(medi) \n",
    "        row_tmp.append(acti)  \n",
    "        row_tmp.append(auth)\n",
    "        row_tmp.append(favo)\n",
    "        row_tmp.append(titl)        \n",
    "        data_raw.append(row_tmp)\n",
    "    \n",
    "    # sort according to time\n",
    "    pddata_raw = pd.DataFrame(data_raw,columns=['time','retweets','followers','mentioned','media',\\\n",
    "                                                'active','author','favourites_count','title'])\n",
    "    pddata_raw = pddata_raw.sort_values(by = 'time')\n",
    "    pddata_raw = pddata_raw.reset_index(drop=True)\n",
    "    pddata_raw['tweets'] = 1                   \n",
    "#     print(pddata_raw)\n",
    "\n",
    "    # reset time to hour index\n",
    "    hour_accu = []\n",
    "    hour_day = []\n",
    "    for index, row in pddata_raw.iterrows():  \n",
    "        p = datetime.datetime.fromtimestamp(row[\"time\"], pst_tz)  \n",
    "        hour_accu.append(((p.month-1)*31+p.day-14)*24+p.hour)\n",
    "        hour_day.append(p.hour)    \n",
    "    pddata_raw[\"time\"] = hour_accu\n",
    "    pddata_raw[\"hour of day\"] = hour_day\n",
    "    \n",
    "    # create a new dataframe with desired form\n",
    "    df = pd.DataFrame([],columns=['hour index','tweets','retweets','followers sum','followers max',\\\n",
    "                                  'mentioned','media','active','author','favourites_count','title'])\n",
    "    df['hour index'] = range(pddata_raw.iloc[len(pddata_raw.index)-1,0]+1)\n",
    "    df['tweets'] = pddata_raw.groupby(\"time\")['tweets'].sum()\n",
    "    df['retweets'] = pddata_raw.groupby(\"time\")['retweets'].sum()\n",
    "    df['followers sum'] = pddata_raw.groupby(\"time\")['followers'].sum()\n",
    "    df['followers max'] = pddata_raw.groupby(\"time\")[\"followers\"].max()\n",
    "    df['mentioned'] = pddata_raw.groupby(\"time\")['mentioned'].sum()\n",
    "    df['media'] = pddata_raw.groupby(\"time\")['media'].sum()\n",
    "    df['active'] = pddata_raw.groupby(\"time\")['active'].mean()  \n",
    "    df['author'] = pddata_raw.groupby(\"time\")['author'].nunique() # count number of not-repeating authors    \n",
    "    df['favourites_count'] = pddata_raw.groupby(\"time\")['favourites_count'].sum()\n",
    "    df['title'] = pddata_raw.groupby(\"time\")['title'].mean()\n",
    "            \n",
    "    # reset index of df\n",
    "    df = df.drop([0]).fillna(0).reset_index(drop=True)\n",
    "\n",
    "    # assign number of tweets of the next hour to be the target value\n",
    "    df_y = df.iloc[1:,1].reset_index(drop=True)\n",
    "    df = df[:len(df_y)]\n",
    "    \n",
    "    return df.iloc[:,1:],df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recipe(df_y, pred_y):\n",
    "    \"\"\"\n",
    "    This function plots fitted values vs true values\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    area = np.pi * (4)**2/4\n",
    "    plt.scatter(df_y, pred_y, s = area)\n",
    "    plt.plot([df_y.min(), df_y.max()], [df_y.min(), df_y.max()], 'k--', lw = 1)\n",
    "    plt.xlabel('true values')\n",
    "    plt.ylabel('fitted values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ols_regression(df, df_y):\n",
    "    X2 = sm.add_constant(df)\n",
    "    y = df_y.as_matrix()\n",
    "    lm = sm.OLS(y, X2).fit()\n",
    "    print(lm.summary())\n",
    "    print(list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(574, 10) (1151, 10)\n",
      "(585, 10) (1736, 10)\n",
      "(585, 10) (2321, 10)\n",
      "(585, 10) (2906, 10)\n",
      "(585, 10) (3491, 10)\n"
     ]
    }
   ],
   "source": [
    "#aggregate all data \n",
    "df, df_y = parse_dataset_ten_features(files[0])\n",
    "for i in range(1,6):\n",
    "    df_temp,df_temp_y=parse_dataset_ten_features(files[i])\n",
    "    df=df.append(df_temp,ignore_index=True)\n",
    "    df_y=df_y.append(df_temp_y,ignore_index=True)\n",
    "    print(df_temp.shape,df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kf = KFold(n_splits=5,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units=[50,100,200,300,500,600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer sizes= 50\n",
      "RMSE_train= 93092.01952988436 RMSE_test= 109192.2180396753\n",
      "\n",
      "hidden layer sizes= 100\n",
      "RMSE_train= 98454.99446626086 RMSE_test= 109470.50550361461\n",
      "\n",
      "hidden layer sizes= 200\n",
      "RMSE_train= 135745.87355179316 RMSE_test= 147830.46142238282\n",
      "\n",
      "hidden layer sizes= 300\n",
      "RMSE_train= 796876.5735459172 RMSE_test= 956341.3975176685\n",
      "\n",
      "hidden layer sizes= 500\n",
      "RMSE_train= 1245878.9194948194 RMSE_test= 1405805.2422495356\n",
      "\n",
      "hidden layer sizes= 600\n",
      "RMSE_train= 1454415.435327531 RMSE_test= 1314767.5455997884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_RMSE2_train=np.zeros(6)\n",
    "avg_RMSE2_test=np.zeros(6)\n",
    "for n1,hidden_layer_sizes in enumerate(hidden_units):\n",
    "    MSE_train=[]\n",
    "    MSE_test=[]\n",
    "    total_train=0\n",
    "    total_test=0\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        X_train= df.iloc[train_index]\n",
    "        y_train= df_y.iloc[train_index]\n",
    "        X_test= df.iloc[test_index]\n",
    "        y_test= df_y.iloc[test_index]\n",
    "        reg = MLPRegressor(hidden_layer_sizes=(hidden_layer_sizes,),activation='relu', solver='adam', alpha=1e-5, random_state=42)            \n",
    "        reg.fit(X_train,y_train)\n",
    "        pred_train = reg.predict(X_train)\n",
    "        pred_test = reg.predict(X_test)\n",
    "        MSE_train.append(mean_squared_error(y_train, pred_train)*len(train_index))\n",
    "        MSE_test.append(mean_squared_error(y_test, pred_test)*len(test_index))\n",
    "        total_train=total_train+len(train_index)\n",
    "        total_test=total_test+len(test_index)\n",
    "    avg_RMSE2_test[n1]=np.sqrt(sum(MSE_test)/total_test)\n",
    "    avg_RMSE2_train[n1]=np.sqrt(sum(MSE_train)/total_train)\n",
    "    print(\"hidden layer sizes=\",hidden_layer_sizes)\n",
    "    print(\"RMSE_train=\",avg_RMSE2_train[n1],\"RMSE_test=\",avg_RMSE2_test[n1])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min RMSE in testset= 109192.2180396753\n",
      "hidden layer sizes= 50\n"
     ]
    }
   ],
   "source": [
    "print(\"min RMSE in testset=\",np.min(avg_RMSE2_test))\n",
    "print(\"hidden layer sizes=\",hidden_units[np.argmin(avg_RMSE2_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE= 1493991680.3029842\n"
     ]
    }
   ],
   "source": [
    "reg = MLPRegressor(hidden_layer_sizes=(hidden_units[np.argmin(avg_RMSE2_test)],),activation='relu', solver='adam', alpha=1e-5, random_state=42) \n",
    "reg.fit(df,df_y)\n",
    "pred = reg.predict(df)\n",
    "print(\"MSE=\",mean_squared_error(df_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer sizes= 50\n",
      "RMSE_train= 870742.3002021678 RMSE_test= 918191.8446974328\n",
      "\n",
      "hidden layer sizes= 100\n",
      "RMSE_train= 759347.6447368438 RMSE_test= 908047.6200404303\n",
      "\n",
      "hidden layer sizes= 200\n",
      "RMSE_train= 1902596.6122279984 RMSE_test= 1746900.2677409204\n",
      "\n",
      "hidden layer sizes= 300\n",
      "RMSE_train= 735796.1332734381 RMSE_test= 858461.2409409373\n",
      "\n",
      "hidden layer sizes= 500\n",
      "RMSE_train= 1907991.7108592603 RMSE_test= 1647432.8412819405\n",
      "\n",
      "hidden layer sizes= 600\n",
      "RMSE_train= 152839.14278786935 RMSE_test= 201978.7064490191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_RMSE3_train=np.zeros(6)\n",
    "avg_RMSE3_test=np.zeros(6)\n",
    "for n1,hidden_layer_sizes in enumerate(hidden_units):\n",
    "    MSE_train=[]\n",
    "    MSE_test=[]\n",
    "    total_train=0\n",
    "    total_test=0\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        X_train= df.iloc[train_index]\n",
    "        y_train= df_y.iloc[train_index]\n",
    "        X_test= df.iloc[test_index]\n",
    "        y_test= df_y.iloc[test_index]\n",
    "        reg = MLPRegressor(hidden_layer_sizes=(50,hidden_layer_sizes,),activation='relu', solver='adam', alpha=1e-5, random_state=42)            \n",
    "        reg.fit(X_train,y_train)\n",
    "        pred_train = reg.predict(X_train)\n",
    "        pred_test = reg.predict(X_test)\n",
    "        MSE_train.append(mean_squared_error(y_train, pred_train)*len(train_index))\n",
    "        MSE_test.append(mean_squared_error(y_test, pred_test)*len(test_index))\n",
    "        total_train=total_train+len(train_index)\n",
    "        total_test=total_test+len(test_index)\n",
    "    avg_RMSE3_test[n1]=np.sqrt(sum(MSE_test)/total_test)\n",
    "    avg_RMSE3_train[n1]=np.sqrt(sum(MSE_train)/total_train)\n",
    "    print(\"hidden layer sizes=\",hidden_layer_sizes)\n",
    "    print(\"RMSE_train=\",avg_RMSE3_train[n1],\"RMSE_test=\",avg_RMSE3_test[n1])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min RMSE in testset= 201978.7064490191\n",
      "hidden layer sizes= 600\n"
     ]
    }
   ],
   "source": [
    "print(\"min RMSE in testset=\",np.min(avg_RMSE3_test))\n",
    "print(\"hidden layer sizes=\",hidden_units[np.argmin(avg_RMSE3_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE= 1435417557274.456\n"
     ]
    }
   ],
   "source": [
    "reg = MLPRegressor(hidden_layer_sizes=(50,hidden_units[np.argmin(avg_RMSE3_test)],),activation='relu', solver='adam', alpha=1e-5, random_state=42) \n",
    "reg.fit(df,df_y)\n",
    "pred = reg.predict(df)\n",
    "print(\"MSE=\",mean_squared_error(df_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer sizes= 50\n",
      "RMSE_train= 605009.2552429812 RMSE_test= 1028205.4110832777\n",
      "\n",
      "hidden layer sizes= 100\n",
      "RMSE_train= 581915.8177173582 RMSE_test= 851870.3980124992\n",
      "\n",
      "hidden layer sizes= 200\n",
      "RMSE_train= 370703.05367547716 RMSE_test= 328678.5981202987\n",
      "\n",
      "hidden layer sizes= 300\n",
      "RMSE_train= 199592.2255822874 RMSE_test= 225363.95533437002\n",
      "\n",
      "hidden layer sizes= 500\n",
      "RMSE_train= 524338.6640544101 RMSE_test= 247449.4941913326\n",
      "\n",
      "hidden layer sizes= 600\n",
      "RMSE_train= 748144.2736463654 RMSE_test= 635336.5516964127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_RMSE4_train=np.zeros(6)\n",
    "avg_RMSE4_test=np.zeros(6)\n",
    "for n1,hidden_layer_sizes in enumerate(hidden_units):\n",
    "    MSE_train=[]\n",
    "    MSE_test=[]\n",
    "    total_train=0\n",
    "    total_test=0\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        X_train= df.iloc[train_index]\n",
    "        y_train= df_y.iloc[train_index]\n",
    "        X_test= df.iloc[test_index]\n",
    "        y_test= df_y.iloc[test_index]\n",
    "        reg = MLPRegressor(hidden_layer_sizes=(50,600,hidden_layer_sizes,),activation='relu', solver='adam', alpha=1e-5, random_state=42)            \n",
    "        reg.fit(X_train,y_train)\n",
    "        pred_train = reg.predict(X_train)\n",
    "        pred_test = reg.predict(X_test)\n",
    "        MSE_train.append(mean_squared_error(y_train, pred_train)*len(train_index))\n",
    "        MSE_test.append(mean_squared_error(y_test, pred_test)*len(test_index))\n",
    "        total_train=total_train+len(train_index)\n",
    "        total_test=total_test+len(test_index)\n",
    "    avg_RMSE4_test[n1]=np.sqrt(sum(MSE_test)/total_test)\n",
    "    avg_RMSE4_train[n1]=np.sqrt(sum(MSE_train)/total_train)\n",
    "    print(\"hidden layer sizes=\",hidden_layer_sizes)\n",
    "    print(\"RMSE_train=\",avg_RMSE4_train[n1],\"RMSE_test=\",avg_RMSE4_test[n1])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min RMSE in testset= 225363.95533437002\n",
      "hidden layer sizes= 300\n"
     ]
    }
   ],
   "source": [
    "print(\"min RMSE in testset=\",np.min(avg_RMSE4_test))\n",
    "print(\"hidden layer sizes=\",hidden_units[np.argmin(avg_RMSE4_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE= 357390845.83744156\n"
     ]
    }
   ],
   "source": [
    "reg = MLPRegressor(hidden_layer_sizes=(50,600,hidden_units[np.argmin(avg_RMSE4_test)],),activation='relu', solver='adam', alpha=1e-5, random_state=42) \n",
    "reg.fit(df,df_y)\n",
    "pred = reg.predict(df)\n",
    "print(\"MSE=\",mean_squared_error(df_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network with Standardscalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE= 9277604.124366865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df)\n",
    "df_trans=scaler.transform(df)\n",
    "reg = MLPRegressor(hidden_layer_sizes=(50,600,300,),activation='relu', solver='adam', alpha=1e-5, random_state=42) \n",
    "reg.fit(df_trans,df_y)\n",
    "pred = reg.predict(df_trans)\n",
    "print(\"MSE=\",mean_squared_error(df_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_regression(df, df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
